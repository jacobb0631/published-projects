{
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "NN with np"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30636,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cupy as cp\n",
        "import pickle\n",
        "from keras.datasets import mnist\n",
        "\n",
        "def load_mnist_dataset():\n",
        "    (train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "    train_images = cp.array(train_images.reshape((train_images.shape[0], 28, 28, 1))).astype('float32') / 255\n",
        "    test_images = cp.array(test_images.reshape((test_images.shape[0], 28, 28, 1))).astype('float32') / 255\n",
        "    return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "def print_progress_bar(iteration, total, prefix='', suffix='', decimals=1, length=50, fill='█'):\n",
        "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
        "    filled_length = int(length * iteration // total)\n",
        "    bar = fill * filled_length + '-' * (length - filled_length)\n",
        "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end='\\r')\n",
        "    # Print New Line on Complete\n",
        "    if iteration == total:\n",
        "        print()\n",
        "\n",
        "train_images, train_labels, test_images, test_labels = load_mnist_dataset()\n",
        "\n",
        "class cnn:\n",
        "    def __init__(self, input_shape, num_filters, filter_size, pool_size, output_size):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_filters = num_filters\n",
        "        self.filter = cp.random.randn(num_filters, filter_size[0], filter_size[1], input_shape[2]) * 0.01\n",
        "        self.pool_size = pool_size\n",
        "\n",
        "        conv_output_height = (input_shape[0] - filter_size[0] + 1) // pool_size[0]\n",
        "        conv_output_width = (input_shape[1] - filter_size[1] + 1) // pool_size[1]\n",
        "\n",
        "        if conv_output_height <= 0 or conv_output_width <= 0:\n",
        "            raise ValueError(\"Invalid dimensions for convolution and pooling layers.\")\n",
        "\n",
        "        flattened_size = conv_output_height * conv_output_width * num_filters\n",
        "        self.weights_fc = cp.random.randn(flattened_size, output_size) * 0.01\n",
        "        self.bias_fc = cp.zeros((1, output_size))\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + cp.exp(-z))\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
        "\n",
        "    def convolve(self, input_images, filter):\n",
        "        batch_size, image_height, image_width, _ = input_images.shape\n",
        "        filter_height, filter_width, _ = filter.shape[1:4]\n",
        "        output_height = image_height - filter_height + 1\n",
        "        output_width = image_width - filter_width + 1\n",
        "\n",
        "        if output_height <= 0 or output_width <= 0:\n",
        "            raise ValueError(\"Invalid dimensions for convolution.\")\n",
        "\n",
        "        result = cp.zeros((batch_size, output_height, output_width, self.num_filters))\n",
        "\n",
        "        for f in range(self.num_filters):\n",
        "            for b in range(batch_size):\n",
        "                for i in range(output_height):\n",
        "                    for j in range(output_width):\n",
        "                        result[b, i, j, f] = cp.sum(\n",
        "                            input_images[b, i:i+filter_height, j:j+filter_width, :] * filter[f]\n",
        "                        )\n",
        "\n",
        "        return result\n",
        "\n",
        "    def pool(self, x):\n",
        "        batch_size, height, width, num_filters = x.shape\n",
        "        pooled_height = height // self.pool_size[0]\n",
        "        pooled_width = width // self.pool_size[1]\n",
        "\n",
        "        result = cp.zeros((batch_size, pooled_height, pooled_width, num_filters))\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            for f in range(num_filters):\n",
        "                for i in range(pooled_height):\n",
        "                    for j in range(pooled_width):\n",
        "                        result[b, i, j, f] = cp.max(\n",
        "                            x[b, i*self.pool_size[0]:(i+1)*self.pool_size[0],\n",
        "                                j*self.pool_size[1]:(j+1)*self.pool_size[1], f]\n",
        "                        )\n",
        "\n",
        "        return result\n",
        "\n",
        "    def flatten(self, x):\n",
        "        return x.reshape(x.shape[0], -1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        if len(img.shape) == 3:\n",
        "            img = img[cp.newaxis, ...]\n",
        "\n",
        "        if img.shape[1:] != self.input_shape:\n",
        "            raise ValueError(f\"Input shape must be {self.input_shape}, but got {img.shape[1:]}\")\n",
        "\n",
        "        conv_output = self.convolve(img, self.filter)\n",
        "        pooled_output = self.pool(conv_output)\n",
        "        flattened_output = self.flatten(pooled_output)\n",
        "        z_fc = cp.dot(flattened_output, self.weights_fc) + self.bias_fc\n",
        "        a_fc = self.sigmoid(z_fc)\n",
        "        return a_fc\n",
        "\n",
        "    def backward(self, x, y, output, learning_rate):\n",
        "        error = output - y\n",
        "        d_weights_fc = cp.dot(self.flatten(self.pool(self.convolve(x, self.filter))).T, error)\n",
        "        self.weights_fc -= learning_rate * d_weights_fc\n",
        "        self.bias_fc -= learning_rate * error\n",
        "\n",
        "    def train(self, x, y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(len(x)):\n",
        "                img = x[i].reshape((1,) + self.input_shape)\n",
        "                output = self.forward(img)\n",
        "                self.backward(img, y[i], output, learning_rate)\n",
        "\n",
        "    def predict(self, x):\n",
        "        if len(x.shape) != 3 or x.shape[1:] != self.input_shape:\n",
        "            raise ValueError(f\"Input shape must be {self.input_shape}, but got {x.shape}\")\n",
        "        output = self.forward(x[cp.newaxis, ...])\n",
        "        return cp.argmax(output, axis=1)\n",
        "\n",
        "    def save_weights(self, filename):\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump([cp.asnumpy(self.filter), cp.asnumpy(self.weights_fc), cp.asnumpy(self.bias_fc)], f)\n",
        "\n",
        "    def load_weights(self, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            self.filter, self.weights_fc, self.bias_fc = [cp.array(arr) for arr in pickle.load(f)]\n",
        "\n",
        "num_labels = 10\n",
        "learning_rate = 0.1\n",
        "epochs = 1\n",
        "train_labels_one_hot = np.eye(num_labels)[train_labels]\n",
        "\n",
        "train_images = cp.array(train_images)\n",
        "train_labels_one_hot = cp.array(train_labels_one_hot)\n",
        "test_images = cp.array(test_images)\n",
        "test_labels = cp.array(test_labels)\n",
        "\n",
        "num_filters = 3\n",
        "filter_size = (3, 3)\n",
        "pool_size = (2, 2)\n",
        "input_shape = (28, 28, 1)\n",
        "output_size = 10\n",
        "\n",
        "network = cnn(input_shape, num_filters, filter_size, pool_size, output_size)\n",
        "\n",
        "total_batches = len(train_images)\n",
        "for epoch in range(epochs):\n",
        "    print(f'Epoch {epoch+1}/{epochs}')\n",
        "    for i in range(0, len(train_images)):\n",
        "        img = train_images[i].reshape((1,) + network.input_shape)\n",
        "        label = train_labels_one_hot[i].reshape((1, -1))\n",
        "        network.train(img, label, 1, learning_rate)\n",
        "        print_progress_bar(i + 1, total_batches, prefix='Progress:', suffix='Complete', length=50)\n",
        "\n",
        "# Testing the network\n",
        "predictions = cp.array([network.predict(img) for img in test_images])\n",
        "accuracy = cp.mean(predictions == test_labels)\n",
        "print('Test Accuracy:', accuracy)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-25T16:51:45.002687Z",
          "iopub.execute_input": "2024-01-25T16:51:45.003202Z"
        },
        "trusted": true,
        "id": "Ws6g3fqad2KD",
        "outputId": "16719e3d-a089-42f2-8e8b-606b94e44e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/1\nProgress: |█████████████-------------------------------------| 27.5% Complete\r",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Button for saving weights\n",
        "save_button = widgets.Button(description=\"Save Weights\")\n",
        "def save_button_clicked(b):\n",
        "    # Specify the filename\n",
        "    network.save_weights(\"network_weights.pkl\")\n",
        "    print(\"Weights saved successfully.\")\n",
        "save_button.on_click(save_button_clicked)\n",
        "\n",
        "# Button for loading weights\n",
        "load_button = widgets.Button(description=\"Load Weights\")\n",
        "def load_button_clicked(b):\n",
        "    # Specify the filename\n",
        "    network.load_weights(\"network_weights.pkl\")\n",
        "    print(\"Weights loaded successfully.\")\n",
        "load_button.on_click(load_button_clicked)\n",
        "\n",
        "# Button for running the model\n",
        "run_button = widgets.Button(description=\"Run Model\")\n",
        "def run_button_clicked(b):\n",
        "    clear_output(wait=True)\n",
        "    indices = random.sample(range(len(test_images)), 10)\n",
        "    for i in range(0, 9):\n",
        "        image = test_images[i].reshape(network.input_shape)\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.show()\n",
        "        output = network.forward(image)\n",
        "        predicted_label = np.argmax(output, axis=1)\n",
        "        print(\"Predicted Label:\", predicted_label[0])\n",
        "run_button.on_click(run_button_clicked)\n",
        "\n",
        "# Display the buttons\n",
        "display(save_button, load_button, run_button)"
      ],
      "metadata": {
        "id": "fKEx_0fg0Yxj",
        "execution": {
          "iopub.status.busy": "2024-01-25T16:22:24.926809Z",
          "iopub.status.idle": "2024-01-25T16:22:24.927137Z",
          "shell.execute_reply.started": "2024-01-25T16:22:24.926976Z",
          "shell.execute_reply": "2024-01-25T16:22:24.926991Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IYZpuqQQ_E2D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}